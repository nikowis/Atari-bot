\documentclass[12pt]{article}

\usepackage[T1]{fontenc}
\usepackage[polish]{babel}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\selectlanguage{polish}

\usepackage{graphicx}
\usepackage{tabularx, booktabs}
\usepackage{fancyhdr} 
\usepackage{geometry}
\usepackage{hyperref}

\usepackage{a4wide}

\geometry{left=15mm,right=25mm,%
bindingoffset=10mm, top=20mm, bottom=20mm}
 


\renewcommand{\maketitle}{
\begin{titlepage}
\begin{table}[t]
\centering
\begin{tabular}[t]{lcr}
 \includegraphics[width=70pt,height=70pt]{PW} & POLITECHNIKA WARSZAWSKA & \includegraphics[width=70pt,height=70pt]{MiNI}\\
& WYDZIAŁ MATEMATYKI & \\
& I NAUK INFORMACYJNYCH &
\end{tabular}
\end{table}
\vspace*{3cm}
  \begin{center}
    \LARGE
    \textbf {MSI2 - Konspekt}\\
   \vspace*{2 cm}
\begin{table}[!htp]
\begin{tabular}{p{4cm}p{9cm}}
\textit{Przedmiot:} &\textbf {Metody sztucznej inteligencji 2} \\
\\
\textit{Projekt:} &\textbf {Agent do grania w gry Atari przy użyciu uczenia wzmocnionego (reinforcement learning)} \\
\\
\textit{Autorzy:} &\textbf {Michał~Kołodziej \newline Nikodem~Wiśniewski} \\
\\
\end{tabular}
\end{table}

\vspace{5 cm}
  \center{\small Warszawa, dnia \today}
\end{center}
\end{titlepage}
}

\begin{document}
\maketitle


\section{Opis i cel projektu}
Zadaniem tego projektu jest stworzenie agenta komputerowego grającego w gry z Atari 2600. Agent będzie trenowany za pomocą uczenia przez wzmacnianie (reinforcement learning) poprzez samoczynne granie w wybrane gry. Agent będzie miał styczność z wieloma grami z konsoli Atari, przy czym po wytrenowaniu się w jednej grze, powinien on szybciej się uczyć grać w kolejną podobną grę. 
Celem projektu jest stworzenie agenta, który nauczy się grać w kilka gier na poziomie jak najbardziej zbliżonym do ludzkiego bądź lepszym.

\section{Planowane modele, pomysły, algorytmy}

\subsection{Uczenie przez wzmocnienie}
Reinforcement learning to obszar uczenia maszynowego inspirowany psychologią behawioralną, w którym agenci podejmują akcje w środowisku, w taki sposób aby maksymalizować jakiegoś rodzaju sumaryczną nagrodę. Początkowo agent posiada wiedze tylko o aktualnym stanie środowiska oraz o możliwych do wykonania akcjach.
\\\
W kontekście realizowanego projektu możemy zdefiniować:
\begin{enumerate}
\item \textbf{Cel agenta}: ukończenie gry z jak największym wynikiem
\item \textbf{Stan środowiska}: zestaw pikseli z ekranu gry
\item \textbf{Akcje}: czynności możliwe do zrealizowania przez gracza
\item \textbf{Nagroda}: zwiększenie wyniku
\item \textbf{Kara}: przegranie gry
\end{enumerate}

\subsection{Architektura programu}
Program będzie składał się z następujących modułów:
\begin{enumerate}
\item \textbf{Moduł komunikacji z użytkownikiem}
\item \textbf{Moduł komunikacji z grami Atari} za pomocą, którego będziemy włączali/wyłączali grę, a także przesyłali informacje o stanie gry do modułu agenta i symulowali akcje podjęte przez agenta w grze.
\item \textbf{Moduł agenta}
\end{enumerate}

\subsection{Deep Q-learning}
Reinforcement learning nie odpowiada na pytanie w jaki sposób wybrać optymalną akcję dla danego stanu środowiska. W tym celu użyta zostanie metoda \textit{deep Q-learning}, która polega na użyciu aproksymatora funkcji, który będzie głęboką siecią neuronową. Metoda ta była użyta przy rozwiązaniu identycznego problemu grania w gry Atari 2600 przez firmę \textit{Google DeepMind}

\section{Opis danych}
Jako dane wejściowe dla agenta, pobierane będą kolejne klatki (zestawy pikseli) gry, w którą będzie aktualnie grać agent. Użycie ostatnich czterech klatek zamiast jednej, przy wyborze następnej akcji przez agenta pozwoli mu na szerszy obraz zmian w środowisku. Danymi wyjściowymi będą czas gry oraz uzyskana przez agenta liczba punktów.

\section{Weryfikacja rezultatów}
Wyniki agenta w danej grze Atari reprezentować będzie liczba punktów, którą uzyska w danej grze. Jako, że celem projektu jest stworzenie agenta, który nauczy się grać na poziomie ludzkim lub lepszym, jego wyniki zostaną porównane z wynikami uzyskanymi przez nas w danych grach, a także z najwyższymi wynikami znalezionymi w internecie (np. ze strony: http://www.jvgs.net/2600/top50.htm). \\ Jeżeli wyniki będą zbliżone do naszych bądź wyższe, ale nie większe od najlepszego wyniku ze strony, którą wybierzemy jako odnośnik do najlepszych wyników, uznamy że agent gra na poziomie ludzkim. Jeśli osiągnie wynik lepszy, uznamy że osiągnął poziom ponad człowiekiem.

\end{document}